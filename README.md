# Reinforcement Learning Research Project

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Pygame](https://img.shields.io/badge/Pygame-2.0+-green.svg)
![NumPy](https://img.shields.io/badge/NumPy-1.21+-orange.svg)
![License](https://img.shields.io/badge/License-Educational-red.svg)

**Äáº¡i há»c BÃ¡ch Khoa TP.HCM - Khoa Khoa há»c vÃ  Ká»¹ thuáº­t MÃ¡y tÃ­nh**

</div>

---

## Tá»•ng quan

Äá»“ Ã¡n nÃ y trÃ¬nh bÃ y viá»‡c nghiÃªn cá»©u vÃ  triá»ƒn khai cÃ¡c thuáº­t toÃ¡n **Há»c TÄƒng CÆ°á»ng (Reinforcement Learning)** thÃ´ng qua hai bÃ i toÃ¡n thá»±c táº¿:

| #   | Project              | Thuáº­t toÃ¡n | MÃ´ táº£                                          |
| --- | -------------------- | ---------- | ---------------------------------------------- |
| 1   | **Grid World**       | Q-Learning | BÃ i toÃ¡n tÃ¬m Ä‘Æ°á»ng cá»• Ä‘iá»ƒn "Chuá»™t tÃ¬m pho mÃ¡t" |
| 2   | **BipedalWalker-v3** | PPO        | Huáº¥n luyá»‡n robot hai chÃ¢n vÆ°á»£t Ä‘á»‹a hÃ¬nh        |

---

## CÆ¡ sá»Ÿ lÃ½ thuyáº¿t

### Reinforcement Learning lÃ  gÃ¬?

Há»c TÄƒng CÆ°á»ng lÃ  má»™t nhÃ¡nh cá»§a Machine Learning trong Ä‘Ã³:

```
Agent â†â†’ Environment
  â†“           â†“
Action â†’ State + Reward
```

- **Agent** (TÃ¡c nhÃ¢n): ÄÆ°a ra quyáº¿t Ä‘á»‹nh
- **Environment** (MÃ´i trÆ°á»ng): Pháº£n há»“i láº¡i hÃ nh Ä‘á»™ng
- **Policy** (ChÃ­nh sÃ¡ch): Chiáº¿n lÆ°á»£c chá»n hÃ nh Ä‘á»™ng
- **Reward** (Pháº§n thÆ°á»Ÿng): TÃ­n hiá»‡u Ä‘Ã¡nh giÃ¡ hÃ nh Ä‘á»™ng

### Lá»‹ch sá»­ phÃ¡t triá»ƒn (Má»¥c 2 - BÃ¡o cÃ¡o)

NhÆ° trÃ¬nh bÃ y trong **DÃ²ng cháº£y "Há»c qua Thá»­ vÃ  Sai"**:

| Tháº­p ká»· | Cá»™t má»‘c quan trá»ng                  |
| ------- | ----------------------------------- |
| 1950s   | Bellman Ä‘á» xuáº¥t Dynamic Programming |
| 1989    | Watkins phÃ¡t minh Q-Learning        |
| 2013    | DeepMind giá»›i thiá»‡u DQN             |
| 2017    | PPO trá»Ÿ thÃ nh chuáº©n má»±c             |
| 2023    | DreamerV3 vÃ  World Models           |

---

## Project 1: Grid World Environment

### MÃ´ táº£ bÃ i toÃ¡n

MÃ´i trÆ°á»ng lÆ°á»›i **4x4** vá»›i 16 tráº¡ng thÃ¡i, nÆ¡i tÃ¡c nhÃ¢n pháº£i tÃ¬m Ä‘Æ°á»ng tá»« **START** Ä‘áº¿n **GOAL** mÃ  khÃ´ng rÆ¡i vÃ o **HOLE**.

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚STARTâ”‚HOLE â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚     â”‚     â”‚HOLE â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚     â”‚HOLE â”‚     â”‚     â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¤
â”‚HOLE â”‚     â”‚     â”‚GOAL â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
```

### ThÃ nh pháº§n MDP (Markov Decision Process)

| ThÃ nh pháº§n      | Chi tiáº¿t                                   |
| --------------- | ------------------------------------------ |
| **States (S)**  | 16 Ã´ vuÃ´ng (0-15)                          |
| **Actions (A)** | 4 hÆ°á»›ng: LEFT, RIGHT, UP, DOWN             |
| **Rewards (R)** | +1.0 (Goal), -1.0 (Hole), -0.01 (má»—i bÆ°á»›c) |
| **Transition**  | Deterministic (xÃ¡c Ä‘á»‹nh)                   |

### CÃ i Ä‘áº·t vÃ  Cháº¡y

```bash
# Clone repository
git clone <repository-url>
cd DATH

# CÃ i Ä‘áº·t dependencies
pip install pygame numpy

# Cháº¡y chÆ°Æ¡ng trÃ¬nh
python grid.py
```

### Äiá»u khiá»ƒn

| PhÃ­m    | Chá»©c nÄƒng              |
| ------- | ---------------------- |
| `SPACE` | Thá»±c hiá»‡n má»™t bÆ°á»›c há»c |
| `ENTER` | Báº­t/táº¯t cháº¿ Ä‘á»™ tá»± Ä‘á»™ng |
| `R`     | Reset toÃ n bá»™ Q-Table  |
| `ESC`   | ThoÃ¡t chÆ°Æ¡ng trÃ¬nh     |

### Tham sá»‘ Hyperparameters

```python
alpha = 0.1      # Tá»‘c Ä‘á»™ há»c (Learning Rate)
gamma = 0.95     # Há»‡ sá»‘ chiáº¿t kháº¥u (Discount Factor)
eps = 0.3        # XÃ¡c suáº¥t khÃ¡m phÃ¡ (Epsilon-greedy)
max_steps = 100  # Giá»›i háº¡n bÆ°á»›c má»—i episode
```

### Chiáº¿n lÆ°á»£c Epsilon-Greedy (Má»¥c 3.1.2)

Theo bÃ¡o cÃ¡o, tÃ¡c nhÃ¢n sá»­ dá»¥ng chiáº¿n lÆ°á»£c cÃ¢n báº±ng giá»¯a:

- **KhÃ¡m phÃ¡ (Exploration):** Chá»n ngáº«u nhiÃªn hÃ nh Ä‘á»™ng vá»›i xÃ¡c suáº¥t Îµ = 0.3
- **Khai thÃ¡c (Exploitation):** Chá»n hÃ nh Ä‘á»™ng cÃ³ Q-value cao nháº¥t vá»›i xÃ¡c suáº¥t 1-Îµ = 0.7

### CÃ´ng thá»©c cáº­p nháº­t Q-Learning

```
Q(s,a) â† Q(s,a) + Î± Ã— [r + Î³ Ã— max(Q(s',a')) - Q(s,a)]
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              TD Error (Î´)
```

### Káº¿t quáº£ thá»±c nghiá»‡m (Báº£ng 1 - BÃ¡o cÃ¡o)

Sau **10.000 episodes** huáº¥n luyá»‡n, Q-Table há»c Ä‘Æ°á»£c cÃ¡c giÃ¡ trá»‹:

| State | Vá»‹ trÃ­ | HÃ nh Ä‘á»™ng tá»‘i Æ°u | Q-Value | "TÆ° duy" cá»§a AI              |
| ----- | ------ | ---------------- | ------- | ---------------------------- |
| 0     | Start  | DOWN             | 0.590   | "ÄÆ°á»ng nÃ y Ä‘iá»ƒm cao nháº¥t"    |
| 4     | (1,0)  | DOWN             | 0.686   | "Tiáº¿p tá»¥c xuá»‘ng lÃ  an toÃ n"  |
| 5     | (1,1)  | RIGHT            | 0.729   | "Ráº½ pháº£i lÃ  Ä‘Æ°á»ng sÃ¡ng nháº¥t" |
| 6     | (1,2)  | DOWN             | 0.806   | "TrÃ¡nh há»‘, Ä‘i xuá»‘ng"         |
| 10    | (2,2)  | RIGHT            | 0.855   | "Äang Ä‘áº¿n gáº§n Ä‘Ã­ch rá»“i"      |
| 14    | (3,2)  | RIGHT            | 1.000   | "NhÃ¬n tháº¥y kho bÃ¡u rá»“i!"     |

---

## Project 2: BipedalWalker-v3 (PPO)

### MÃ´ táº£ bÃ i toÃ¡n (Má»¥c 3.2)

Huáº¥n luyá»‡n robot hai chÃ¢n há»c cÃ¡ch Ä‘i bá»™ vÃ  vÆ°á»£t Ä‘á»‹a hÃ¬nh báº±ng thuáº­t toÃ¡n **Proximal Policy Optimization (PPO)**.

Notebook Link: https://www.kaggle.com/code/phmquanghiu/doantonghop1

Notebook Link: https://www.kaggle.com/code/phmquanghiu/doantonghop

### KhÃ´ng gian tráº¡ng thÃ¡i (24 chiá»u)

| Index | ThÃ´ng tin             | MÃ´ táº£                   |
| ----- | --------------------- | ----------------------- |
| 0     | Hull Angle            | GÃ³c nghiÃªng thÃ¢n        |
| 1     | Hull Angular Velocity | Váº­n tá»‘c gÃ³c             |
| 2-3   | Velocity (x, y)       | Váº­n tá»‘c ngang/dá»c       |
| 4-13  | Joint States          | GÃ³c vÃ  váº­n tá»‘c cÃ¡c khá»›p |
| 14-23 | LIDAR                 | 10 cáº£m biáº¿n khoáº£ng cÃ¡ch |

### KhÃ´ng gian hÃ nh Ä‘á»™ng (4 chiá»u liÃªn tá»¥c)

| Index | Khá»›p      | Pháº¡m vi | MÃ´ táº£                   |
| ----- | --------- | ------- | ----------------------- |
| 0     | HÃ´ng trÃ¡i | [-1, 1] | Äiá»u khiá»ƒn Ä‘Ã¹i trÃ¡i     |
| 1     | Gá»‘i trÃ¡i  | [-1, 1] | Duá»—i/gáº­p cáº³ng chÃ¢n trÃ¡i |
| 2     | HÃ´ng pháº£i | [-1, 1] | Äiá»u khiá»ƒn Ä‘Ã¹i pháº£i     |
| 3     | Gá»‘i pháº£i  | [-1, 1] | Duá»—i/gáº­p cáº³ng chÃ¢n pháº£i |

### Há»‡ thá»‘ng Reward

```
Reward = Tiáº¿n vá» phÃ­a trÆ°á»›c (+)
       - Chi phÃ­ nÄƒng lÆ°á»£ng (-)
       - Pháº¡t ngÃ£ (-100)
       + Bonus hoÃ n thÃ nh (+300)
```

### QuÃ¡ trÃ¬nh há»™i tá»¥ (HÃ¬nh 10 - BÃ¡o cÃ¡o)

| Giai Ä‘oáº¡n      | Episodes | Reward     | MÃ´ táº£              |
| -------------- | -------- | ---------- | ------------------ |
| **BÃ¹ng ná»•**    | 0-200    | -100 â†’ +50 | Robot "xÃ³a mÃ¹ chá»¯" |
| **Tinh chá»‰nh** | 200-1000 | Dao Ä‘á»™ng   | Giá»¯a ~300 vÃ  ~-100 |
| **á»”n Ä‘á»‹nh**    | >1000    | ~250-300   | Äi vá»¯ng vÃ ng       |

---

## CÃ¡c thuáº­t toÃ¡n nÃ¢ng cao (Má»¥c 4)

### DreamerV3 vÃ  World Models

NhÆ° trÃ¬nh bÃ y trong bÃ¡o cÃ¡o, cÃ¡c thuáº­t toÃ¡n hiá»‡n Ä‘áº¡i sá»­ dá»¥ng:

- **World Model**: MÃ´ hÃ¬nh ná»™i táº¡i vá» mÃ´i trÆ°á»ng
- **Imagination**: Há»c trong "giáº¥c mÆ¡" mÃ  khÃ´ng cáº§n tÆ°Æ¡ng tÃ¡c thá»±c
- **Sample Efficiency**: Hiá»‡u quáº£ máº«u cao hÆ¡n

---

## á»¨ng dá»¥ng thá»±c tiá»…n (Má»¥c 5)

| LÄ©nh vá»±c       | á»¨ng dá»¥ng         | VÃ­ dá»¥                  |
| -------------- | ---------------- | ---------------------- |
| **Game AI**    | NPC thÃ´ng minh   | AlphaGo, OpenAI Five   |
| **Robotics**   | Äiá»u khiá»ƒn robot | Boston Dynamics        |
| **Finance**    | Trading tá»± Ä‘á»™ng  | Portfolio Optimization |
| **Healthcare** | Tá»‘i Æ°u Ä‘iá»u trá»‹  | Drug Discovery         |

---

## ThÃ¡ch thá»©c vÃ  Xu hÆ°á»›ng (Má»¥c 6)

### ThÃ¡ch thá»©c hiá»‡n táº¡i

1. **Sample Inefficiency** - Cáº§n nhiá»u dá»¯ liá»‡u huáº¥n luyá»‡n
2. **Reward Shaping** - Thiáº¿t káº¿ hÃ m reward khÃ³
3. **Sim-to-Real Gap** - KhÃ¡c biá»‡t mÃ´ phá»ng vs thá»±c táº¿
4. **Safety** - Äáº£m báº£o an toÃ n khi triá»ƒn khai

### Xu hÆ°á»›ng tÆ°Æ¡ng lai

- **Offline RL** - Há»c tá»« dá»¯ liá»‡u cÃ³ sáºµn
- **Multi-Agent RL** - Nhiá»u agent phá»‘i há»£p
- **Foundation Models** - MÃ´ hÃ¬nh ná»n táº£ng cho RL

---

## Cáº¥u trÃºc thÆ° má»¥c

```
DATH/
â”œâ”€â”€ ğŸ“„ grid.py              # Project 1: Q-Learning GridWorld
â”œâ”€â”€ ğŸ“„ README.md            # File hÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ ğŸ“ Report/
    â”œâ”€â”€ ğŸ“„ main.tex         # BÃ¡o cÃ¡o LaTeX chÃ­nh
    â”œâ”€â”€ ğŸ“„ main.pdf         # BÃ¡o cÃ¡o PDF Ä‘Ã£ biÃªn dá»‹ch
    â”œâ”€â”€ ğŸ“„ references.bib   # TÃ i liá»‡u tham kháº£o

```

---

## TÃ i liá»‡u tham kháº£o

1. Sutton, R. S., & Barto, A. G. (2018). _Reinforcement Learning: An Introduction_ (2nd ed.). MIT Press.

2. Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. _Machine Learning_, 8(3-4), 279-292.

3. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). _Proximal Policy Optimization Algorithms_. arXiv:1707.06347.

4. Hafner, D., Pasukonis, J., Ba, J., & Lillicrap, T. (2023). _Mastering Diverse Domains through World Models_. arXiv:2301.04104.

5. Mnih, V., et al. (2015). Human-level control through deep reinforcement learning. _Nature_, 518(7540), 529-533.

---

## Ghi chÃº

- Äá»“ Ã¡n Ä‘Æ°á»£c thá»±c hiá»‡n cho má»¥c Ä‘Ã­ch **há»c táº­p** táº¡i Äáº¡i há»c BÃ¡ch Khoa TP.HCM
- Code Ä‘Æ°á»£c viáº¿t báº±ng **Python 3.8+**
- Visualization sá»­ dá»¥ng **Pygame** Ä‘á»ƒ trá»±c quan hÃ³a quÃ¡ trÃ¬nh há»c

---

<div align="center">

**Â© 2024 - Pháº¡m Quang Hiáº¿u - ÄHBK TP.HCM**

_"The only way to learn is by doing."_ - Richard Sutton

</div>
