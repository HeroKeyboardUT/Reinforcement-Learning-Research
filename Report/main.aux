\relax 
\bibstyle{biblatex}
\bibdata{main-blx,references}
\citation{biblatex-control}
\abx@aux@refcontext{none/global//global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{symbols}{slg}{sls}{slo}
\@newglossary{abbreviations}{glg-abr}{gls-abr}{glo-abr}
\providecommand\@glsxtr@savepreloctag[2]{}
\pgfsyspdfmark {pgfid1}{6856841}{46244836}
\citation{sutton2018reinforcement}
\abx@aux@cite{0}{sutton2018reinforcement}
\abx@aux@segm{0}{0}{sutton2018reinforcement}
\citation{datarootlabs_rl_state}
\abx@aux@cite{0}{datarootlabs_rl_state}
\abx@aux@segm{0}{0}{datarootlabs_rl_state}
\citation{datarootlabs_rl_state}
\abx@aux@cite{0}{datarootlabs_rl_state}
\abx@aux@segm{0}{0}{datarootlabs_rl_state}
\@writefile{toc}{\contentsline {section}{\numberline {1}Giới thiệu}{3}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Vòng lặp tương tác cơ bản giữa Tác nhân và Môi trường trong RL.}}{3}{figure.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Những thành tựu nổi bật về Reinforcement Learning}{3}{subsection.1.1}\protected@file@percent }
\citation{sutton2018reinforcement}
\abx@aux@cite{0}{sutton2018reinforcement}
\abx@aux@segm{0}{0}{sutton2018reinforcement}
\citation{britannica_law_effect}
\abx@aux@cite{0}{britannica_law_effect}
\abx@aux@segm{0}{0}{britannica_law_effect}
\citation{britannica_law_effect}
\abx@aux@cite{0}{britannica_law_effect}
\abx@aux@segm{0}{0}{britannica_law_effect}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Sơ bộ về bài báo cáo}{4}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Lịch sử và Cội nguồn Tư tưởng của Học Tăng Cường}{4}{section.2}\protected@file@percent }
\newlabel{sec:history_narrative}{{2}{4}{Lịch sử và Cội nguồn Tư tưởng của Học Tăng Cường}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Dòng chảy thứ nhất: Học qua Thử và Sai – Bản năng của sinh tồn}{4}{subsection.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Hộp Skinner (Skinner Box):} Thí nghiệm sinh học kinh điển minh họa cơ chế cốt lõi của Học tăng cường.\blx@tocontentsinit {0}\cite {britannica_law_effect}}}{5}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:skinner_box}{{2}{5}{\textbf {Hộp Skinner (Skinner Box):} Thí nghiệm sinh học kinh điển minh họa cơ chế cốt lõi của Học tăng cường.\cite {britannica_law_effect}}{figure.caption.5}{}}
\citation{TD}
\abx@aux@cite{0}{TD}
\abx@aux@segm{0}{0}{TD}
\citation{TD}
\abx@aux@cite{0}{TD}
\abx@aux@segm{0}{0}{TD}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Dòng chảy thứ hai: Bài toán Optimal Control – Sự chính xác của toán học}{6}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Dòng chảy thứ ba: Temporal-Difference Learning – Chiếc cầu nối bị lãng quên}{6}{subsection.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces So sánh ba phương pháp học: Dynamic Programming (phải biết toàn bộ mô hình), Monte Carlo (học từ kết quả cuối), và Temporal-Difference (học từng bước ngay lập tức). \blx@tocontentsinit {0}\cite {TD}}}{7}{figure.caption.6}\protected@file@percent }
\newlabel{fig:td_comparison}{{3}{7}{So sánh ba phương pháp học: Dynamic Programming (phải biết toàn bộ mô hình), Monte Carlo (học từ kết quả cuối), và Temporal-Difference (học từng bước ngay lập tức). \cite {TD}}{figure.caption.6}{}}
\citation{sutton2018reinforcement}
\abx@aux@cite{0}{sutton2018reinforcement}
\abx@aux@segm{0}{0}{sutton2018reinforcement}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Sự hội tụ và Kỷ nguyên mới của Reinforcement learning}{8}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Ý tưởng cốt lõi của Reinforcement Learning}{8}{subsection.2.5}\protected@file@percent }
\citation{researchgate_rl_category}
\abx@aux@cite{0}{researchgate_rl_category}
\abx@aux@segm{0}{0}{researchgate_rl_category}
\citation{researchgate_rl_category}
\abx@aux@cite{0}{researchgate_rl_category}
\abx@aux@segm{0}{0}{researchgate_rl_category}
\newlabel{eq:v_function_def}{{5}{9}{Ý tưởng cốt lõi của Reinforcement Learning}{equation.2.5}{}}
\newlabel{eq:q_function_def}{{6}{9}{Ý tưởng cốt lõi của Reinforcement Learning}{equation.2.6}{}}
\newlabel{eq:bellman_optimality}{{7}{9}{Ý tưởng cốt lõi của Reinforcement Learning}{equation.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Các phương pháp huấn luyện}{9}{subsection.2.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Phân loại các thuật toán RL phổ biến. \blx@tocontentsinit {0}\cite {researchgate_rl_category}}}{9}{figure.caption.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.1}Phương pháp Dựa trên Giá trị (Value-based Methods)}{9}{subsubsection.2.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.2}Phương pháp Dựa trên Chính sách (Policy-based Methods)}{10}{subsubsection.2.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.3}Phương pháp Actor-Critic}{10}{subsubsection.2.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Kiến trúc Actor-Critic: Cả hai mạng cùng nhận Input là State, nhưng Actor đưa ra hành động còn Critic đưa ra đánh giá giá trị.}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:actor_critic}{{5}{11}{Kiến trúc Actor-Critic: Cả hai mạng cùng nhận Input là State, nhưng Actor đưa ra hành động còn Critic đưa ra đánh giá giá trị}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.6.4}Phương pháp Dựa trên Mô hình (Model-Based Methods)}{11}{subsubsection.2.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Các ví dụ về Reinforcement Learning và dự án thực hành)}{12}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Mô phỏng Q-Learning với Grid World: "Chuột tìm pho mát"}{12}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Thiết lập bài toán}{12}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Quá trình học chi tiết: "Cuốn sổ tay kinh nghiệm"}{12}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Minh họa Môi trường Grid World}{13}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Mô phỏng môi trường lưới. Tác nhân (Start) phải tìm đường nét đứt màu xanh để đến Đích (Goal) mà không rơi vào Hố (Hole).}}{13}{figure.caption.9}\protected@file@percent }
\newlabel{fig:grid_world}{{6}{13}{Mô phỏng môi trường lưới. Tác nhân (Start) phải tìm đường nét đứt màu xanh để đến Đích (Goal) mà không rơi vào Hố (Hole)}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Minh họa tính toán}{14}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.5}Kết quả thực nghiệm: Giải mã Q-Table sau 10.000 tập}{14}{subsubsection.3.1.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Phân tích "tư duy" của AI dựa trên số liệu thực tế}}{15}{table.caption.10}\protected@file@percent }
\newlabel{tab:q_analysis_real}{{1}{15}{Phân tích "tư duy" của AI dựa trên số liệu thực tế}{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.6}Diễn biến quá trình học: Từ "Tờ giấy trắng" đến "Bản đồ kho báu"}{15}{subsubsection.3.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.7}Phân tích sự tổng quát}{16}{subsubsection.3.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Project 2: Bipedal Walker (Normal Mode) - Huấn luyện Robot vượt địa hình bằng PPO}{16}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Tổng quan bài toán và Lý do lựa chọn giải pháp}{16}{subsubsection.3.2.1}\protected@file@percent }
\citation{kaviani2023deepmprenhancingopportunisticrouting}
\abx@aux@cite{0}{kaviani2023deepmprenhancingopportunisticrouting}
\abx@aux@segm{0}{0}{kaviani2023deepmprenhancingopportunisticrouting}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Ảnh minh họa project}}{17}{figure.caption.11}\protected@file@percent }
\newlabel{fig:placeholder}{{7}{17}{Ảnh minh họa project}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Lý do chọn thuật toán PPO (Proximal Policy Optimization)}{17}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Cơ sở lý thuyết: Nguyên lý hoạt động của PPO\blx@tocontentsinit {0}\cite {kaviani2023deepmprenhancingopportunisticrouting}}{17}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces PPO Algorithm}}{18}{figure.caption.12}\protected@file@percent }
\citation{gym_bipedalwalker}
\abx@aux@cite{0}{gym_bipedalwalker}
\abx@aux@segm{0}{0}{gym_bipedalwalker}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}Phân tích chi tiết Đặc tả Môi trường (Environment Specification)\blx@tocontentsinit {0}\cite {gym_bipedalwalker}}{19}{subsubsection.3.2.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Chi tiết vector trạng thái 24 chiều của BipedalWalker-v3 với miền giá trị cụ thể}}{19}{table.caption.13}\protected@file@percent }
\newlabel{tab:obs_detail}{{2}{19}{Chi tiết vector trạng thái 24 chiều của BipedalWalker-v3 với miền giá trị cụ thể}{table.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Đặc tả chi tiết Vector hành động điều khiển khớp}}{20}{table.caption.14}\protected@file@percent }
\newlabel{tab:action_space}{{3}{20}{Đặc tả chi tiết Vector hành động điều khiển khớp}{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Phân tích trạng thái leo bậc thang của Robot: Cần sự phối hợp giữa gập gối và đẩy hông.}}{20}{figure.caption.15}\protected@file@percent }
\newlabel{fig:robot_stairs}{{9}{20}{Phân tích trạng thái leo bậc thang của Robot: Cần sự phối hợp giữa gập gối và đẩy hông}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.5}Thực nghiệm và Đánh giá Kết quả}{21}{subsubsection.3.2.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Biểu đồ quá trình hội tụ của PPO trên môi trường BipedalWalker-v3 (1 triệu bước). Trục tung là điểm thưởng, trục hoành là số tập chơi.}}{22}{figure.caption.16}\protected@file@percent }
\newlabel{fig:training_curve}{{10}{22}{Biểu đồ quá trình hội tụ của PPO trên môi trường BipedalWalker-v3 (1 triệu bước). Trục tung là điểm thưởng, trục hoành là số tập chơi}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Với PPO thô, robot có xu hướng bước đi với những bước chân ngắn}}{23}{figure.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Tối ưu hóa Mô hình và So sánh Hiệu quả}{24}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Cấu hình Tối ưu (Optimized Configuration)}{24}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Bảng so sánh cấu hình huấn luyện giữa Baseline và Optimized}}{24}{table.caption.19}\protected@file@percent }
\newlabel{tab:comparison}{{4}{24}{Bảng so sánh cấu hình huấn luyện giữa Baseline và Optimized}{table.caption.19}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Phân tích sự thay đổi Hành vi (Behavior Analysis)}{24}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Kết quả Thực nghiệm và Đánh giá Hiệu năng}{25}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Thống kê hiệu năng của mô hình Optimized tại mốc 1 triệu bước}}{25}{table.caption.20}\protected@file@percent }
\newlabel{tab:final_log}{{5}{25}{Thống kê hiệu năng của mô hình Optimized tại mốc 1 triệu bước}{table.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Minh họa bước đi}{26}{subsubsection.3.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Robot Optimized thể hiện chiến thuật sải bước dài (Long Stride) và dứt khoát hơn}}{26}{figure.caption.22}\protected@file@percent }
\newlabel{fig:long_stride}{{12}{26}{Robot Optimized thể hiện chiến thuật sải bước dài (Long Stride) và dứt khoát hơn}{figure.caption.22}{}}
\citation{ouyang2022training}
\abx@aux@cite{0}{ouyang2022training}
\abx@aux@segm{0}{0}{ouyang2022training}
\@writefile{toc}{\contentsline {section}{\numberline {4}Điểm qua một số nghiên cứu đột phá có liên quan đến Reinforcement Learning}{27}{section.4}\protected@file@percent }
\newlabel{sec:review_paper}{{4}{27}{Điểm qua một số nghiên cứu đột phá có liên quan đến Reinforcement Learning}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}InstructGPT (2022): Căn chỉnh mô hình ngôn ngữ bằng phản hồi con người \blx@tocontentsinit {0}\cite {ouyang2022training}}{27}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Diagram mô tả từng bước trong phương pháp huấn luyện mô hình(ảnh từ paper của OpenAI)}}{28}{figure.caption.23}\protected@file@percent }
\citation{deepseek2025incentivizing}
\abx@aux@cite{0}{deepseek2025incentivizing}
\abx@aux@segm{0}{0}{deepseek2025incentivizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025)\blx@tocontentsinit {0}\cite {deepseek2025incentivizing}}{29}{subsection.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Biểu đồ về hiệu năng của Deepseek (ảnh từ paper của Deepseek)}}{29}{figure.caption.24}\protected@file@percent }
\citation{DreamerV3}
\abx@aux@cite{0}{DreamerV3}
\abx@aux@segm{0}{0}{DreamerV3}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Mastering Diverse Domains through World Models (2024)\blx@tocontentsinit {0}\cite {DreamerV3}}{30}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Hiệu suất của DreamerV3 so với các thuật toán chuyên dụng. a) DreamerV3 (cột xanh đậm) vượt trội trên các miền từ Atari, ProcGen đến điều khiển Robot với cùng một bộ tham số cố định. b) Biểu đồ thể hiện khả năng thu thập Kim cương trong Minecraft, một tác vụ mà các thuật toán trước đó thất bại nếu không có dữ liệu mẫu.}}{31}{figure.caption.25}\protected@file@percent }
\newlabel{fig:dreamerv3}{{15}{31}{Hiệu suất của DreamerV3 so với các thuật toán chuyên dụng. a) DreamerV3 (cột xanh đậm) vượt trội trên các miền từ Atari, ProcGen đến điều khiển Robot với cùng một bộ tham số cố định. b) Biểu đồ thể hiện khả năng thu thập Kim cương trong Minecraft, một tác vụ mà các thuật toán trước đó thất bại nếu không có dữ liệu mẫu}{figure.caption.25}{}}
\citation{datarootlabs_rl_state}
\abx@aux@cite{0}{datarootlabs_rl_state}
\abx@aux@segm{0}{0}{datarootlabs_rl_state}
\citation{datarootlabs_rl_state}
\abx@aux@cite{0}{datarootlabs_rl_state}
\abx@aux@segm{0}{0}{datarootlabs_rl_state}
\@writefile{toc}{\contentsline {section}{\numberline {5}Reinforcement Learning: Ứng dụng trong thực tiễn}{32}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Hệ sinh thái Đổi mới và Ứng dụng Đa ngành (Innovation Landscape)}{32}{subsection.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Các lĩnh vực ứng dụng chính của Reinforcement Learning (ảnh từ Dataroot Labs \blx@tocontentsinit {0}\cite {datarootlabs_rl_state})}}{32}{figure.caption.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Reinforcement Learning và phạm vi sử dụng}{33}{subsection.5.2}\protected@file@percent }
\citation{datarootlabs_rl_state}
\abx@aux@cite{0}{datarootlabs_rl_state}
\abx@aux@segm{0}{0}{datarootlabs_rl_state}
\citation{milvus_rl_limitations}
\abx@aux@cite{0}{milvus_rl_limitations}
\abx@aux@segm{0}{0}{milvus_rl_limitations}
\citation{artiba_rl_future}
\abx@aux@cite{0}{artiba_rl_future}
\abx@aux@segm{0}{0}{artiba_rl_future}
\@writefile{toc}{\contentsline {section}{\numberline {6}Hạn chế và tương lai của Reinforcement Learning}{34}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Những thách thức cốt lõi (Key Challenges) \blx@tocontentsinit {0}\cite {datarootlabs_rl_state} \blx@tocontentsinit {0}\cite {milvus_rl_limitations}}{34}{subsection.6.1}\protected@file@percent }
\citation{multi_agent_rl}
\abx@aux@cite{0}{multi_agent_rl}
\abx@aux@segm{0}{0}{multi_agent_rl}
\citation{multi_agent_rl}
\abx@aux@cite{0}{multi_agent_rl}
\abx@aux@segm{0}{0}{multi_agent_rl}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Xu hướng và Định hướng phát triển (Future Directions) \blx@tocontentsinit {0}\cite {artiba_rl_future}}{35}{subsection.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Minh họa về Single-Agent và Multi-Agent RL \blx@tocontentsinit {0}\cite {multi_agent_rl}}}{35}{figure.caption.27}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Kết luận}{35}{section.7}\protected@file@percent }
\newlabel{sec:conclusion}{{7}{35}{Kết luận}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Mã nguồn và Tài nguyên Thực nghiệm}{36}{appendix.A}\protected@file@percent }
\newlabel{appendix:source_code}{{A}{36}{Mã nguồn và Tài nguyên Thực nghiệm}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Project 1: Grid World Environment}{36}{subsection.A.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Project 2: BipedalWalker-v3 (Continuous Control)}{36}{subsection.A.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {B}Tài liệu tham khảo}{36}{appendix.B}\protected@file@percent }
\newlabel{LastPage}{{}{37}{}{page.37}{}}
\xdef\lastpage@lastpage{37}
\xdef\lastpage@lastpageHy{37}
\abx@aux@read@bbl@mdfivesum{31DBC34D7E4C4733E2FD19EFCB291BA0}
\abx@aux@defaultrefcontext{0}{sutton2018reinforcement}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{datarootlabs_rl_state}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{britannica_law_effect}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{TD}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{researchgate_rl_category}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{kaviani2023deepmprenhancingopportunisticrouting}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{gym_bipedalwalker}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{ouyang2022training}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{deepseek2025incentivizing}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{DreamerV3}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{milvus_rl_limitations}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{artiba_rl_future}{none/global//global/global}
\abx@aux@defaultrefcontext{0}{multi_agent_rl}{none/global//global/global}
\abx@aux@defaultlabelprefix{0}{sutton2018reinforcement}{}
\abx@aux@defaultlabelprefix{0}{datarootlabs_rl_state}{}
\abx@aux@defaultlabelprefix{0}{britannica_law_effect}{}
\abx@aux@defaultlabelprefix{0}{TD}{}
\abx@aux@defaultlabelprefix{0}{researchgate_rl_category}{}
\abx@aux@defaultlabelprefix{0}{kaviani2023deepmprenhancingopportunisticrouting}{}
\abx@aux@defaultlabelprefix{0}{gym_bipedalwalker}{}
\abx@aux@defaultlabelprefix{0}{ouyang2022training}{}
\abx@aux@defaultlabelprefix{0}{deepseek2025incentivizing}{}
\abx@aux@defaultlabelprefix{0}{DreamerV3}{}
\abx@aux@defaultlabelprefix{0}{milvus_rl_limitations}{}
\abx@aux@defaultlabelprefix{0}{artiba_rl_future}{}
\abx@aux@defaultlabelprefix{0}{multi_agent_rl}{}
\gdef \@abspage@last{38}
