\contentsline {section}{\numberline {1}Giới thiệu}{3}{section.1}%
\contentsline {subsection}{\numberline {1.1}Những thành tựu nổi bật về Reinforcement Learning}{3}{subsection.1.1}%
\contentsline {subsection}{\numberline {1.2}Sơ bộ về bài báo cáo}{4}{subsection.1.2}%
\contentsline {section}{\numberline {2}Lịch sử và Cội nguồn Tư tưởng của Học Tăng Cường}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Dòng chảy thứ nhất: Học qua Thử và Sai – Bản năng của sinh tồn}{4}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Dòng chảy thứ hai: Bài toán Optimal Control – Sự chính xác của toán học}{6}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Dòng chảy thứ ba: Temporal-Difference Learning – Chiếc cầu nối bị lãng quên}{6}{subsection.2.3}%
\contentsline {subsection}{\numberline {2.4}Sự hội tụ và Kỷ nguyên mới của Reinforcement learning}{8}{subsection.2.4}%
\contentsline {subsection}{\numberline {2.5}Ý tưởng cốt lõi của Reinforcement Learning}{8}{subsection.2.5}%
\contentsline {subsection}{\numberline {2.6}Các phương pháp huấn luyện}{9}{subsection.2.6}%
\contentsline {subsubsection}{\numberline {2.6.1}Phương pháp Dựa trên Giá trị (Value-based Methods)}{9}{subsubsection.2.6.1}%
\contentsline {subsubsection}{\numberline {2.6.2}Phương pháp Dựa trên Chính sách (Policy-based Methods)}{10}{subsubsection.2.6.2}%
\contentsline {subsubsection}{\numberline {2.6.3}Phương pháp Actor-Critic}{10}{subsubsection.2.6.3}%
\contentsline {subsubsection}{\numberline {2.6.4}Phương pháp Dựa trên Mô hình (Model-Based Methods)}{11}{subsubsection.2.6.4}%
\contentsline {section}{\numberline {3}Các ví dụ về Reinforcement Learning và dự án thực hành)}{12}{section.3}%
\contentsline {subsection}{\numberline {3.1}Mô phỏng Q-Learning với Grid World: "Chuột tìm pho mát"}{12}{subsection.3.1}%
\contentsline {subsubsection}{\numberline {3.1.1}Thiết lập bài toán}{12}{subsubsection.3.1.1}%
\contentsline {subsubsection}{\numberline {3.1.2}Quá trình học chi tiết: "Cuốn sổ tay kinh nghiệm"}{12}{subsubsection.3.1.2}%
\contentsline {subsubsection}{\numberline {3.1.3}Minh họa Môi trường Grid World}{13}{subsubsection.3.1.3}%
\contentsline {subsubsection}{\numberline {3.1.4}Minh họa tính toán}{14}{subsubsection.3.1.4}%
\contentsline {subsubsection}{\numberline {3.1.5}Kết quả thực nghiệm: Giải mã Q-Table sau 10.000 tập}{14}{subsubsection.3.1.5}%
\contentsline {subsubsection}{\numberline {3.1.6}Diễn biến quá trình học: Từ "Tờ giấy trắng" đến "Bản đồ kho báu"}{15}{subsubsection.3.1.6}%
\contentsline {subsubsection}{\numberline {3.1.7}Phân tích sự tổng quát}{16}{subsubsection.3.1.7}%
\contentsline {subsection}{\numberline {3.2}Project 2: Bipedal Walker (Normal Mode) - Huấn luyện Robot vượt địa hình bằng PPO}{16}{subsection.3.2}%
\contentsline {subsubsection}{\numberline {3.2.1}Tổng quan bài toán và Lý do lựa chọn giải pháp}{16}{subsubsection.3.2.1}%
\contentsline {subsubsection}{\numberline {3.2.2}Lý do chọn thuật toán PPO (Proximal Policy Optimization)}{17}{subsubsection.3.2.2}%
\contentsline {subsubsection}{\numberline {3.2.3}Cơ sở lý thuyết: Nguyên lý hoạt động của PPO\blx@tocontentsinit {0}\cite {kaviani2023deepmprenhancingopportunisticrouting}}{17}{subsubsection.3.2.3}%
\contentsline {subsubsection}{\numberline {3.2.4}Phân tích chi tiết Đặc tả Môi trường (Environment Specification)\blx@tocontentsinit {0}\cite {gym_bipedalwalker}}{19}{subsubsection.3.2.4}%
\contentsline {subsubsection}{\numberline {3.2.5}Thực nghiệm và Đánh giá Kết quả}{21}{subsubsection.3.2.5}%
\contentsline {subsection}{\numberline {3.3}Tối ưu hóa Mô hình và So sánh Hiệu quả}{24}{subsection.3.3}%
\contentsline {subsubsection}{\numberline {3.3.1}Cấu hình Tối ưu (Optimized Configuration)}{24}{subsubsection.3.3.1}%
\contentsline {subsubsection}{\numberline {3.3.2}Phân tích sự thay đổi Hành vi (Behavior Analysis)}{24}{subsubsection.3.3.2}%
\contentsline {subsubsection}{\numberline {3.3.3}Kết quả Thực nghiệm và Đánh giá Hiệu năng}{25}{subsubsection.3.3.3}%
\contentsline {subsubsection}{\numberline {3.3.4}Minh họa bước đi}{26}{subsubsection.3.3.4}%
\contentsline {section}{\numberline {4}Điểm qua một số nghiên cứu đột phá có liên quan đến Reinforcement Learning}{27}{section.4}%
\contentsline {subsection}{\numberline {4.1}InstructGPT (2022): Căn chỉnh mô hình ngôn ngữ bằng phản hồi con người \blx@tocontentsinit {0}\cite {ouyang2022training}}{27}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning (2025)\blx@tocontentsinit {0}\cite {deepseek2025incentivizing}}{29}{subsection.4.2}%
\contentsline {subsection}{\numberline {4.3}Mastering Diverse Domains through World Models (2024)\blx@tocontentsinit {0}\cite {DreamerV3}}{30}{subsection.4.3}%
\contentsline {section}{\numberline {5}Reinforcement Learning: Ứng dụng trong thực tiễn}{32}{section.5}%
\contentsline {subsection}{\numberline {5.1}Hệ sinh thái Đổi mới và Ứng dụng Đa ngành (Innovation Landscape)}{32}{subsection.5.1}%
\contentsline {subsection}{\numberline {5.2}Reinforcement Learning và phạm vi sử dụng}{33}{subsection.5.2}%
\contentsline {section}{\numberline {6}Hạn chế và tương lai của Reinforcement Learning}{34}{section.6}%
\contentsline {subsection}{\numberline {6.1}Những thách thức cốt lõi (Key Challenges) \blx@tocontentsinit {0}\cite {datarootlabs_rl_state} \blx@tocontentsinit {0}\cite {milvus_rl_limitations}}{34}{subsection.6.1}%
\contentsline {subsection}{\numberline {6.2}Xu hướng và Định hướng phát triển (Future Directions) \blx@tocontentsinit {0}\cite {artiba_rl_future}}{35}{subsection.6.2}%
\contentsline {section}{\numberline {7}Kết luận}{35}{section.7}%
\contentsline {section}{\numberline {A}Code đầy đủ project}{36}{appendix.A}%
\contentsline {section}{\numberline {B}Tài liệu tham khảo}{36}{appendix.B}%
